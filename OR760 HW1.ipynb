{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def coins_w_sensor(sims):\n",
    "\n",
    "    flips = [(random.choice(['H', 'T']), random.choice(['H', 'T'])) for _ in range(sims)]   # Simulate two coin flips\n",
    "    pairs = ['equal' if flip[0] == flip[1] else 'different' for flip in flips]  # Determine if they are equal or different, instead of 0/1\n",
    "    #print(pairs)\n",
    "\n",
    "    reports = []\n",
    "    for i in range(len(pairs)):\n",
    "        R = random.random()\n",
    "        #print(R)\n",
    "        if R <= 0.9:    # 90% chance to report correctly\n",
    "            reports.append(pairs[i])\n",
    "        else:\n",
    "            reports.append('equal' if pairs[i] == 'different' else 'different')\n",
    "\n",
    "    return pairs, reports\n",
    "\n",
    "pair, report = coins_w_sensor(10**6) # 10^6 simulations\n",
    "#print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(R=1) (report equal) = 0.500226\n",
      "P(equal | R=1) = 0.89933550035384\n"
     ]
    }
   ],
   "source": [
    "print(f\"P(R=1) (report equal) = {report.count('equal') / len(report)}\")\n",
    "print(f\"P(equal | R=1) = {sum(1 for i in range(len(report)) if report[i] == 'equal' and pair[i] == 'equal') / report.count('equal')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.\n",
    "\n",
    "The differences between the values above and the analytic values found in Part I are very close, differing absolutely by only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute difference to P(R=1): 0.00022599999999994846\n",
      "Absolute difference to P(equal | R=1): 0.0006644996461599995\n"
     ]
    }
   ],
   "source": [
    "print(f\"Absolute difference to P(R=1): {np.abs(0.5 - report.count('equal') / len(report))}\")\n",
    "print(f\"Absolute difference to P(equal | R=1): {np.abs(0.9 - sum(1 for i in range(len(report)) if report[i] == 'equal' and pair[i] == 'equal') / report.count('equal'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\n",
    "\n",
    "Conditioning on a noisy report can change posterior probabibilities significantly even with a low noise rate because the probability space shifts to dealing with new events. In our example, the original probability is 50/50, but once we knew that one of the events had occurred, the test was (still) 90% accurate. Even if the original probability was much lower, say 1%, once the report has occurred, we are now considering the probability of true positives versus false positives, which is NOT the original probability space of the event in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
